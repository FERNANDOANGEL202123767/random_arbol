import os
from flask import Flask, jsonify, request, render_template, send_from_directory
from flask_cors import CORS
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_squared_error, r2_score, f1_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
import io
import base64
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Global variable to store the dataset
global_data = None

def load_dataset():
    """Load the dataset with error handling"""
    global global_data
    try:
        # Ensure the data directory exists
        data_dir = os.path.join(os.path.dirname(__file__), 'data')
        os.makedirs(data_dir, exist_ok=True)
        
        # Look for the CSV file
        csv_path = os.path.join(data_dir, 'archivo_reducido.csv')
        
        if not os.path.exists(csv_path):
            logger.error(f"Dataset file not found at {csv_path}")
            raise FileNotFoundError(f"Dataset file not found at {csv_path}")
        
        global_data = pd.read_csv(csv_path)
        
        # Validate dataset
        validate_dataset(global_data)
        
        logger.info("Dataset loaded successfully")
        return global_data
    except Exception as e:
        logger.error(f"Error loading dataset: {e}")
        raise

def validate_dataset(df):
    """Validate the dataset structure"""
    required_columns = ['class']  # Corrected spelling
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"Missing required columns: {missing_columns}")
    
    # Additional validation if needed
    if len(df) == 0:
        raise ValueError("Dataset is empty")

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/action/load_data', methods=['GET'])
def load_data():
    try:
        if global_data is None:
            load_dataset()
        
        # Convert the first few rows to HTML
        data_html = global_data.head().to_html(classes='table table-striped')
        return jsonify({'data': data_html})
    except Exception as e:
        logger.error(f"Error in load_data: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/action/length_features', methods=['GET'])
def length_features():
    try:
        if global_data is None:
            load_dataset()
        
        length = len(global_data)
        features = len(global_data.columns) - 1  # Assuming the last column is the target
        return jsonify({
            'length': length,
            'features': features
        })
    except Exception as e:
        logger.error(f"Error in length_features: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/action/split_scale', methods=['GET'])
def split_scale():
    try:
        if global_data is None:
            load_dataset()
        
        X = global_data.drop('class', axis=1)  # Corrected column name
        y = global_data['class']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # Convert the first few rows of scaled data to HTML
        scaled_data_html = pd.DataFrame(X_train_scaled, columns=X.columns).head().to_html(classes='table table-striped')
        return jsonify({'scaled_data': scaled_data_html})
    except Exception as e:
        logger.error(f"Error in split_scale: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/action/train_tree', methods=['GET'])
def train_tree():
    try:
        if global_data is None:
            load_dataset()
        
        X = global_data.drop('class', axis=1)
        y = global_data['class']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        
        clf = DecisionTreeClassifier(random_state=42)
        clf.fit(X_train, y_train)
        
        y_train_pred = clf.predict(X_train)
        y_test_pred = clf.predict(X_test)
        
        f1_train = f1_score(y_train, y_train_pred, average='weighted')
        f1_val = f1_score(y_test, y_test_pred, average='weighted')
        
        return jsonify({
            'f1_train': f1_train,
            'f1_val': f1_val
        })
    except Exception as e:
        logger.error(f"Error in train_tree: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/train', methods=['POST'])
def train():
    try:
        if global_data is None:
            load_dataset()
        
        X = global_data.drop('class', axis=1)
        y = global_data['class']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        
        model = RandomForestRegressor(random_state=42)
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        feature_importance = [
            {"caracteristica": feature, "importancia": importance}
            for feature, importance in zip(X.columns, model.feature_importances_)
        ]
        
        # Generate plot
        plt.figure(figsize=(10, 6))
        plt.scatter(y_test, y_pred)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        plt.xlabel('Valores Reales')
        plt.ylabel('Predicciones')
        plt.title('Predicciones vs Valores Reales')
        
        # Save plot to base64
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png')
        buffer.seek(0)
        plot_data = base64.b64encode(buffer.getvalue()).decode('utf-8')
        plt.close()
        
        return jsonify({
            'mse': mse,
            'r2': r2,
            'feature_importance': feature_importance,
            'image': plot_data
        })
    except Exception as e:
        logger.error(f"Error in train: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/decision_boundary', methods=['GET'])
def decision_boundary():
    try:
        # Placeholder for decision boundary visualization
        plt.figure(figsize=(10, 6))
        plt.title('Decision Boundary')
        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png')
        buffer.seek(0)
        plot_data = base64.b64encode(buffer.getvalue()).decode('utf-8')
        plt.close()
        
        return jsonify({'image': plot_data})
    except Exception as e:
        logger.error(f"Error in decision_boundary: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/decision_tree', methods=['GET'])
def decision_tree_visualization():
    try:
        # Placeholder for decision tree visualization
        plt.figure(figsize=(20, 10))
        plt.title('Decision Tree')
        
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png')
        buffer.seek(0)
        plot_data = base64.b64encode(buffer.getvalue()).decode('utf-8')
        plt.close()
        
        return jsonify({'image': plot_data})
    except Exception as e:
        logger.error(f"Error in decision_tree: {e}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 5000))
    app.run(host='0.0.0.0', port=port, debug=True)
